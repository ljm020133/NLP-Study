[*BERT.pdf](file:///C:/Users/gigm2/OneDrive/Desktop/BERT.pdf)

- The paper introduces BERT, a new model for natural language understanding that can learn from unlabeled text and be fine-tuned for different tasks.
- BERT uses a bidirectional Transformer encoder, which can capture both left and right context of each word in a sentence. This allows BERT to learn deep and rich representations of words and sentences.
- [BERT is pre-trained on two tasks: masked language modeling and next sentence prediction](https://edgeservices.bing.com/edgesvc/chat?udsframed=1&form=SHORUN&clientscopes=chat,noheader,udsedgeshop,channelstable,&shellsig=a6d3a26afe8514d995b883e94237379c85077b51&setlang=en-US&lightschemeovr=1#sjevt%7CDiscover.Chat.SydneyClickPageCitation%7Cadpclick%7C0%7C507c6049-5740-47fb-91ed-54fac05f9d13%7C%7B%22sourceAttributions%22%3A%7B%22providerDisplayName%22%3A%22Instead%2C%20w...%22%2C%22pageType%22%3A%22pdf%22%2C%22pageIndex%22%3A4%2C%22relatedPageUrl%22%3A%22file%253A%252F%252F%252FC%253A%252FUsers%252Fgigm2%252FOneDrive%252FDesktop%252FBERT.pdf%22%2C%22lineIndex%22%3A31%2C%22highlightText%22%3A%22Instead%2C%20we%20pre-train%20BERT%20using%20two%20unsuper%5Cu0002vised%20tasks%2C%20described%20in%20this%20section.%22%2C%22snippets%22%3A%5B%5D%7D%7D)[1](https://edgeservices.bing.com/edgesvc/chat?udsframed=1&form=SHORUN&clientscopes=chat,noheader,udsedgeshop,channelstable,&shellsig=a6d3a26afe8514d995b883e94237379c85077b51&setlang=en-US&lightschemeovr=1#sjevt%7CDiscover.Chat.SydneyClickPageCitation%7Cadpclick%7C0%7C507c6049-5740-47fb-91ed-54fac05f9d13%7C%7B%22sourceAttributions%22%3A%7B%22providerDisplayName%22%3A%22Instead%2C%20w...%22%2C%22pageType%22%3A%22pdf%22%2C%22pageIndex%22%3A4%2C%22relatedPageUrl%22%3A%22file%253A%252F%252F%252FC%253A%252FUsers%252Fgigm2%252FOneDrive%252FDesktop%252FBERT.pdf%22%2C%22lineIndex%22%3A31%2C%22highlightText%22%3A%22Instead%2C%20we%20pre-train%20BERT%20using%20two%20unsuper%5Cu0002vised%20tasks%2C%20described%20in%20this%20section.%22%2C%22snippets%22%3A%5B%5D%7D%7D). Masked language modeling randomly hides some words in a sentence and asks the model to predict them. Next sentence prediction asks the model to determine if two sentences are consecutive or not.
- [BERT can be easily adapted to various downstream tasks, such as question answering, natural language inference, sentiment analysis, and named entity recognition, by adding a task-specific output layer and fine-tuning all parameters](https://edgeservices.bing.com/edgesvc/chat?udsframed=1&form=SHORUN&clientscopes=chat,noheader,udsedgeshop,channelstable,&shellsig=a6d3a26afe8514d995b883e94237379c85077b51&setlang=en-US&lightschemeovr=1#sjevt%7CDiscover.Chat.SydneyClickPageCitation%7Cadpclick%7C1%7C507c6049-5740-47fb-91ed-54fac05f9d13%7C%7B%22sourceAttributions%22%3A%7B%22providerDisplayName%22%3A%22As%20a%20re%5Cu0002su...%22%2C%22pageType%22%3A%22pdf%22%2C%22pageIndex%22%3A1%2C%22relatedPageUrl%22%3A%22file%253A%252F%252F%252FC%253A%252FUsers%252Fgigm2%252FOneDrive%252FDesktop%252FBERT.pdf%22%2C%22lineIndex%22%3A11%2C%22highlightText%22%3A%22As%20a%20re%5Cu0002sult%2C%20the%20pre-trained%20BERT%20model%20can%20be%20fine%5Cu0002tuned%20with%20just%20one%20additional%20output%20layer%5Cr%5Cnto%20create%20state-of-the-art%20models%20for%20a%20wide%5Cr%5Cnrange%20of%20tasks%2C%20such%20as%20question%20answering%20and%5Cr%5Cnlanguage%20inference%2C%20without%20substantial%20task%5Cu0002specific%20architecture%20modifications.%22%2C%22snippets%22%3A%5B%5D%7D%7D)[2](https://edgeservices.bing.com/edgesvc/chat?udsframed=1&form=SHORUN&clientscopes=chat,noheader,udsedgeshop,channelstable,&shellsig=a6d3a26afe8514d995b883e94237379c85077b51&setlang=en-US&lightschemeovr=1#sjevt%7CDiscover.Chat.SydneyClickPageCitation%7Cadpclick%7C1%7C507c6049-5740-47fb-91ed-54fac05f9d13%7C%7B%22sourceAttributions%22%3A%7B%22providerDisplayName%22%3A%22As%20a%20re%5Cu0002su...%22%2C%22pageType%22%3A%22pdf%22%2C%22pageIndex%22%3A1%2C%22relatedPageUrl%22%3A%22file%253A%252F%252F%252FC%253A%252FUsers%252Fgigm2%252FOneDrive%252FDesktop%252FBERT.pdf%22%2C%22lineIndex%22%3A11%2C%22highlightText%22%3A%22As%20a%20re%5Cu0002sult%2C%20the%20pre-trained%20BERT%20model%20can%20be%20fine%5Cu0002tuned%20with%20just%20one%20additional%20output%20layer%5Cr%5Cnto%20create%20state-of-the-art%20models%20for%20a%20wide%5Cr%5Cnrange%20of%20tasks%2C%20such%20as%20question%20answering%20and%5Cr%5Cnlanguage%20inference%2C%20without%20substantial%20task%5Cu0002specific%20architecture%20modifications.%22%2C%22snippets%22%3A%5B%5D%7D%7D).
- [BERT achieves state-of-the-art results on eleven natural language processing tasks, outperforming previous models by a large margin](https://edgeservices.bing.com/edgesvc/chat?udsframed=1&form=SHORUN&clientscopes=chat,noheader,udsedgeshop,channelstable,&shellsig=a6d3a26afe8514d995b883e94237379c85077b51&setlang=en-US&lightschemeovr=1#sjevt%7CDiscover.Chat.SydneyClickPageCitation%7Cadpclick%7C2%7C507c6049-5740-47fb-91ed-54fac05f9d13%7C%7B%22sourceAttributions%22%3A%7B%22providerDisplayName%22%3A%22%E2%80%A2%20BERT%20adv...%22%2C%22pageType%22%3A%22pdf%22%2C%22pageIndex%22%3A2%2C%22relatedPageUrl%22%3A%22file%253A%252F%252F%252FC%253A%252FUsers%252Fgigm2%252FOneDrive%252FDesktop%252FBERT.pdf%22%2C%22lineIndex%22%3A15%2C%22highlightText%22%3A%22%E2%80%A2%20BERT%20advances%20the%20state%20of%20the%20art%20for%20eleven%5Cr%5CnNLP%20tasks.%22%2C%22snippets%22%3A%5B%5D%7D%7D)[3](https://edgeservices.bing.com/edgesvc/chat?udsframed=1&form=SHORUN&clientscopes=chat,noheader,udsedgeshop,channelstable,&shellsig=a6d3a26afe8514d995b883e94237379c85077b51&setlang=en-US&lightschemeovr=1#sjevt%7CDiscover.Chat.SydneyClickPageCitation%7Cadpclick%7C2%7C507c6049-5740-47fb-91ed-54fac05f9d13%7C%7B%22sourceAttributions%22%3A%7B%22providerDisplayName%22%3A%22%E2%80%A2%20BERT%20adv...%22%2C%22pageType%22%3A%22pdf%22%2C%22pageIndex%22%3A2%2C%22relatedPageUrl%22%3A%22file%253A%252F%252F%252FC%253A%252FUsers%252Fgigm2%252FOneDrive%252FDesktop%252FBERT.pdf%22%2C%22lineIndex%22%3A15%2C%22highlightText%22%3A%22%E2%80%A2%20BERT%20advances%20the%20state%20of%20the%20art%20for%20eleven%5Cr%5CnNLP%20tasks.%22%2C%22snippets%22%3A%5B%5D%7D%7D). BERT also shows that pre-training on large amounts of unlabeled text can improve the performance of natural language understanding systems.
Transfer leaning(전이학습)
	훈련 한번 하고 파인튜닝을 함으로써 다른 태스크도 수행을 할 수 있도록 하는것.
		EX) 단어 예측하는걸 위해 만든 모델이 베이스 모델이 같다보니깐 내가 원하는 태스크(질문 문답, paraphrasing 등등)의 정확성을 높인다던지; 커스터마이징 한다는 느낌

Fine-tuning
	같은 베이스 모델을 사용한다는 이점이 있고, 같은 베이스 모델을 기반으로 내가 원하는 태스크를 더 잘하게끔 커스터마이징하는 느낌

버트는 트랜스포머에 인코더만 사용, GPT는 디코더만 사용
	인코더:전체 문장을 집어넣기때문에 문장을 표현하거나 전체 문맥을 이해하는거는 잘하지만 반대로 디코더의 태스크(마스크드를 넣는거)는 잘 못함. 다음 나올 단어 예측이나 그런거, 그래서 반대로 GPT는 문맥파악는 못한다던지 하지만 단어를 쭈루룩 이어나가는거는 잘함

https://github.com/coaxsoft/pytorch_bert/tree/master/bert
https://coaxsoft.com/blog/building-bert-with-pytorch-from-scratch
https://colab.research.google.com/drive/11tnCVxdQOvBvsDit2rw09wNNaCsQh_HN#scrollTo=q9R_d-DYn9v5
https://browse.arxiv.org/pdf/2303.18223.pdf


